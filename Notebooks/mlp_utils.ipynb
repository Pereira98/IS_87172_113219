{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1391bb0d-7972-4faa-bde7-31f76abaea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "\n",
    "from utils import import_data,combine_data,split_data,standardize,compute_avg_delta_dataset #standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504bb80-0e91-42af-94e7-8186ca19b242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define MLP Model ---\n",
    "ACTIVATIONS = {\n",
    "    \"RL\": nn.ReLU,\n",
    "    \"TN\": nn.Tanh,\n",
    "    \"SI\": nn.Sigmoid,\n",
    "    \"LI\": nn.Identity\n",
    "}\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, architecture):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "\n",
    "        for elem in architecture:\n",
    "            if isinstance(elem, int):  # layer size\n",
    "                layers.append(nn.Linear(prev, elem))\n",
    "                prev = elem\n",
    "            elif isinstance(elem, str):  # activation\n",
    "                act_class = ACTIVATIONS.get(elem.upper())\n",
    "                if act_class is None:\n",
    "                    raise ValueError(f\"Unknown activation code: {elem}\")\n",
    "                layers.append(act_class())\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid architecture element: {elem}\")\n",
    "\n",
    "        layers.append(nn.Linear(prev, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35f0f6-d1a6-4096-916b-3cfb0e0bbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Function ---\n",
    "def train_mlp(X_train, y_train, X_val, y_val, input_dim, output_dim, \n",
    "              hidden_layers, lr, batch_size, epochs):\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val   = torch.tensor(y_val, dtype=torch.float32)\n",
    "    \n",
    "    model = MLP(input_dim, output_dim, hidden_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_val)\n",
    "        val_loss = criterion(val_pred, y_val).item()\n",
    "    \n",
    "    return model, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20ed8b-83ef-485c-b823-898acf7c346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "def run_kfold_cv(X, y, input_dim, output_dim,\n",
    "                 hidden_layers, lr, batch_size, epochs,\n",
    "                 k=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform K-fold cross-validation using train_mlp().\n",
    "    Uses your standardize() to scale each fold's data safely.\n",
    "    Returns mean and std of validation losses.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    fold_losses = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # --- scale within this fold (fit on train only) ---\n",
    "        X_train, X_val, _, y_train, y_val, _, _, _ = standardize(\n",
    "            X_train, X_val, X_val,  # dummy X_test\n",
    "            y_train, y_val, y_val   # dummy y_test\n",
    "        )\n",
    "\n",
    "        # --- train ---\n",
    "        _, val_loss = train_mlp(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            input_dim, output_dim,\n",
    "            hidden_layers, lr, batch_size, epochs\n",
    "        )\n",
    "\n",
    "        fold_losses.append(val_loss)\n",
    "        print(f\"Fold {fold+1}/{k}: Val Loss = {val_loss:.6f}\")\n",
    "\n",
    "    mean_loss = np.mean(fold_losses)\n",
    "    std_loss  = np.std(fold_losses)\n",
    "    print(f\"\\n📊 Mean CV Loss: {mean_loss:.6f} ± {std_loss:.6f}\")\n",
    "    return mean_loss, std_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef3a94-647d-46c6-a2f0-b18127d04c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grid Search Function ---\n",
    "def grid_search_mlp(X_train, y_train, X_val, y_val, input_dim, output_dim, param_grid):\n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    for combination in product(*values):\n",
    "        params = dict(zip(keys, combination))\n",
    "        print(f\"Testing: {params}\")\n",
    "\n",
    "        model, val_loss= train_mlp(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        results.append((params, val_loss))\n",
    "\n",
    "        # Keep best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "\n",
    "    results.sort(key=lambda x: x[1])\n",
    "    return results, best_model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7612b9-2b6d-40e4-9016-eba9df33ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "def grid_search_mlp_cv(X, y, input_dim, output_dim, param_grid,\n",
    "                       default_k_folds=5, random_state=42, idx=None):\n",
    "    \"\"\"\n",
    "    Perform grid search with K-fold cross-validation using run_kfold_cv().\n",
    "    \n",
    "    Returns:\n",
    "        best_params: dict of best hyperparameters (with folds included if present)\n",
    "        best_loss: lowest mean CV loss found\n",
    "        results: list of dicts with {'params', 'mean_loss', 'std_loss'}\n",
    "        best_model: model retrained on all data using best_params\n",
    "        x_scaler, y_scaler: scalers fitted on full data\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    best_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "\n",
    "    # --- Optional feature filtering ---\n",
    "    if idx is not None:\n",
    "        X = X[:, idx]\n",
    "        print(f\"🧩 Using {X.shape[1]} selected features (indices from feature selection)\")\n",
    "        input_dim = X.shape[1]  # update input dimension automatically\n",
    "\n",
    "    # --- Grid Search Loop ---\n",
    "    for combination in product(*values):\n",
    "        params = dict(zip(keys, combination))\n",
    "        k_folds = params.pop(\"k_folds\", default_k_folds)  # use fold count if present\n",
    "\n",
    "        print(f\"\\n🔍 Testing params: {params} | k_folds={k_folds}\")\n",
    "\n",
    "        # --- Run CV for this combination ---\n",
    "        mean_loss, std_loss = run_kfold_cv(\n",
    "            X, y,\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "            hidden_layers=params[\"hidden_layers\"],\n",
    "            lr=params[\"lr\"],\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            epochs=params[\"epochs\"],\n",
    "            k=k_folds,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"params\": dict(params, k_folds=k_folds),\n",
    "            \"mean_loss\": mean_loss,\n",
    "            \"std_loss\": std_loss\n",
    "        })\n",
    "\n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            best_params = dict(params, k_folds=k_folds)\n",
    "\n",
    "    # --- Report best combination ---\n",
    "    print(f\"\\n🏆 Best parameters found: {best_params}\")\n",
    "    print(f\"   Mean CV Loss: {best_loss:.6f}\")\n",
    "\n",
    "    # --- Retrain final model using best params on full dataset ---\n",
    "    X_train, _, _, y_train, _, _, x_scaler, y_scaler = standardize(X, X, X, y, y, y)\n",
    "    best_model, _ = train_mlp(\n",
    "        X_train, y_train, X_train, y_train,\n",
    "        input_dim=input_dim, output_dim=output_dim,\n",
    "        **{k: v for k, v in best_params.items() if k != \"k_folds\"}\n",
    "    )\n",
    "\n",
    "    return best_params, best_loss, results, best_model, x_scaler, y_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab2b2e-5556-4951-9618-6fdf062d906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(X_train, X_val, input_dim, hidden_layers,\n",
    "                      lr=1e-3, batch_size=32, epochs=100, verbose=False):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "    model = MLP(input_dim, input_dim, hidden_layers)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(X_train, X_train),\n",
    "        batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, _ in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            recon = model(xb)\n",
    "            loss = criterion(recon, xb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if verbose and (epoch+1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = criterion(model(X_val), X_val).item()\n",
    "            print(f\"[AE] Epoch {epoch+1:03d} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = criterion(model(X_val), X_val).item()\n",
    "    return model, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85cb9b3-3960-4606-8245-d8ca1b58a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_encoder(autoencoder_model):\n",
    "    layers = []\n",
    "    for layer in autoencoder_model.model:\n",
    "        layers.append(layer)\n",
    "        if isinstance(layer, nn.Tanh):  # stop at bottleneck activation\n",
    "            break\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def get_latent_dim(encoder):\n",
    "    # find last Linear layer and return its out_features\n",
    "    for layer in reversed(encoder):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            return layer.out_features\n",
    "    raise ValueError(\"Could not find a Linear layer in encoder.\")\n",
    "\n",
    "def train_encoder_regressor(encoder, X_train, y_train, X_val, y_val,\n",
    "                            reg_layers=[32, \"RL\"], lr=1e-3, batch_size=32,\n",
    "                            epochs=100, fine_tune=False, verbose=False):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val   = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    latent_dim = get_latent_dim(encoder)  # ✅ fixed line\n",
    "\n",
    "    class EncoderRegressor(nn.Module):\n",
    "        def __init__(self, encoder, reg_layers, output_dim):\n",
    "            super().__init__()\n",
    "            self.encoder = encoder\n",
    "            self.regressor = MLP(latent_dim, output_dim, reg_layers)\n",
    "        def forward(self, x):\n",
    "            z = self.encoder(x)\n",
    "            return self.regressor(z)\n",
    "\n",
    "    model = EncoderRegressor(encoder, reg_layers, y_train.shape[1])\n",
    "\n",
    "    # Freeze encoder unless fine_tune=True\n",
    "    for p in model.encoder.parameters():\n",
    "        p.requires_grad = fine_tune\n",
    "\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(X_train, y_train),\n",
    "        batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if verbose and (epoch+1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = criterion(model(X_val), y_val).item()\n",
    "            print(f\"[REG] Epoch {epoch+1:03d} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = criterion(model(X_val), y_val).item()\n",
    "    return model, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20ee54-a77d-4690-b65f-21fced5f1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_autoencoder(X_train, y_train, X_val, y_val, input_dim, output_dim, param_grid):\n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "\n",
    "    for combo in product(*values):\n",
    "        params = dict(zip(keys, combo))\n",
    "        print(f\"\\n🔍 Testing config:\\n{params}\")\n",
    "\n",
    "        # 1. Train AE\n",
    "        ae_model, ae_val_loss = train_autoencoder(\n",
    "            X_train, X_val,\n",
    "            input_dim=input_dim,\n",
    "            hidden_layers=params['ae_layers'],\n",
    "            lr=params['lr'],\n",
    "            batch_size=params['batch_size'],\n",
    "            epochs=params['ae_epochs'],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # 2. Extract encoder\n",
    "        encoder = extract_encoder(ae_model)\n",
    "\n",
    "        # 3. Train regressor\n",
    "        reg_model, reg_val_loss = train_encoder_regressor(\n",
    "            encoder, X_train, y_train, X_val, y_val,\n",
    "            reg_layers=params['reg_layers'],\n",
    "            lr=params['lr'],\n",
    "            batch_size=params['batch_size'],\n",
    "            epochs=params['reg_epochs'],\n",
    "            fine_tune=params['fine_tune'],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        total_val_loss = reg_val_loss\n",
    "        print(f\"→ AE Val Loss: {ae_val_loss:.6f} | REG Val Loss: {reg_val_loss:.6f}\")\n",
    "\n",
    "        results.append((params, total_val_loss))\n",
    "        if total_val_loss < best_loss:\n",
    "            best_loss = total_val_loss\n",
    "            best_model = reg_model\n",
    "            best_params = params\n",
    "\n",
    "    results.sort(key=lambda x: x[1])\n",
    "    print(\"\\n✅ Best config:\", best_params)\n",
    "    print(f\"✅ Best Val Loss: {best_loss:.6f}\")\n",
    "    return results, best_model, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061443fe-12ae-4d4b-aca4-631f365eede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hierarchical Model (uses your MLP class) ---\n",
    "class HierarchicalMLP(nn.Module):\n",
    "    def __init__(self, mlp_class=MLP, pair_arch=[8,\"RL\",4,\"RL\"], mid_arch=[8,\"RL\",4,\"RL\"], final_arch=[16,\"RL\"]):\n",
    "        super().__init__()\n",
    "        self.pairs = nn.ModuleList([mlp_class(2, 1, pair_arch) for _ in range(6)])  # 6×(2→1)\n",
    "        self.mids  = nn.ModuleList([mlp_class(3, 3, mid_arch) for _ in range(2)])   # 2×(3→3)\n",
    "        self.final = mlp_class(6, 3, final_arch)                                   # 6→3\n",
    "\n",
    "    def forward(self, x):\n",
    "        pair_outs = [self.pairs[i](x[:, 2*i:2*i+2]) for i in range(6)]\n",
    "        mid1 = self.mids[0](torch.cat(pair_outs[:3], dim=1))\n",
    "        mid2 = self.mids[1](torch.cat(pair_outs[3:], dim=1))\n",
    "        return self.final(torch.cat([mid1, mid2], dim=1))\n",
    "\n",
    "from itertools import product\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "def grid_search_mlp_multi(X_train, y_train, X_val, y_val, param_grid):\n",
    "    results, best_loss, best_model, best_params = [], float('inf'), None, None\n",
    "    keys, values = zip(*param_grid.items())\n",
    "\n",
    "    # Convert to tensors once\n",
    "    X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val,   y_val   = torch.tensor(X_val, dtype=torch.float32),   torch.tensor(y_val, dtype=torch.float32)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for combo in product(*values):\n",
    "        params = dict(zip(keys, combo))\n",
    "        print(f\"Testing: {params}\")\n",
    "\n",
    "        # ✅ Build model directly\n",
    "        model = HierarchicalMLP(\n",
    "            pair_arch=params.get(\"pair_arch\", [8, \"RL\", 4, \"RL\"]),\n",
    "            mid_arch=params.get(\"mid_arch\", [8, \"RL\", 4, \"RL\"]),\n",
    "            final_arch=params.get(\"final_arch\", [16, \"RL\"])\n",
    "        )\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params.get(\"lr\", 1e-3))\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X_train, y_train),\n",
    "            batch_size=params.get(\"batch_size\", 32), shuffle=True\n",
    "        )\n",
    "\n",
    "        # --- Train ---\n",
    "        for _ in range(params.get(\"epochs\", 100)):\n",
    "            for xb, yb in loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(xb), yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # --- Validate ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = criterion(model(X_val), y_val).item()\n",
    "\n",
    "        results.append((params, val_loss))\n",
    "        if val_loss < best_loss:\n",
    "            best_loss, best_model, best_params = val_loss, model, params\n",
    "\n",
    "    results.sort(key=lambda x: x[1])\n",
    "    return results, best_model, best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

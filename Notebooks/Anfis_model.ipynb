{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bce966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded C:\\Users\\user\\Desktop\\si_project\\dataset1.mat using scipy.io\n",
      "Loaded C:\\Users\\user\\Desktop\\si_project\\dataset2.mat using scipy.io\n",
      "Loaded C:\\Users\\user\\Desktop\\si_project\\dataset3.mat using scipy.io\n",
      "\n",
      "All loaded datasets: ['dataset1', 'dataset2', 'dataset3']\n",
      " dataset1: mag_sensors (2000, 12), tip_position (2000, 3)\n",
      " dataset2: mag_sensors2 (2000, 12), tip_position2 (2000, 3)\n",
      " dataset3: mag_sensors3 (2000, 12), tip_position3 (2000, 3)\n",
      "\n",
      " Combined shapes -> X: (6000, 12), y: (6000, 3)\n",
      "   sensor_1  sensor_2  sensor_3  sensor_4  sensor_5  sensor_6  sensor_7  \\\n",
      "0   15544.0   15471.0   13962.0   13618.0   13691.0   13941.0   15588.0   \n",
      "1   14100.0   14144.0   14144.0   13692.0   13338.0   13517.0   13987.0   \n",
      "2   14240.0   14266.0   14445.0   13859.0   13618.0   13756.0   14080.0   \n",
      "3   14956.0   14949.0   14123.0   13707.0   13765.0   13954.0   14554.0   \n",
      "4   14196.0   14246.0   16177.0   14830.0   13836.0   13911.0   13988.0   \n",
      "\n",
      "   sensor_8  sensor_9  sensor_10  sensor_11  sensor_12          x          y  \\\n",
      "0   15234.0   14156.0    13758.0    14098.0    14296.0  71.417546  -4.340464   \n",
      "1   13951.0   14162.0    13774.0    13784.0    13931.0  24.514493  47.778850   \n",
      "2   14036.0   14509.0    13958.0    13996.0    14122.0   2.951708  24.908397   \n",
      "3   14404.0   14208.0    13798.0    14127.0    14282.0  46.508669  -0.243722   \n",
      "4   13984.0   15788.0    14690.0    14185.0    14257.0 -41.687301  34.183930   \n",
      "\n",
      "            z  \n",
      "0   76.317760  \n",
      "1  110.173076  \n",
      "2  112.971805  \n",
      "3  100.567155  \n",
      "4   90.795163  \n",
      "Total subsets with almost all sensors: 79\n",
      "Example of first subsets: [('sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10'), ('sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11'), ('sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_12'), ('sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_10', 'sensor_11'), ('sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_10', 'sensor_12')]\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Imports and file paths\n",
    "# =========================================================\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "project_folder = r\"C:\\Users\\user\\Desktop\\si_project\"\n",
    "\n",
    "mat_files = [\n",
    "    os.path.join(project_folder, 'dataset1.mat'),\n",
    "    os.path.join(project_folder, 'dataset2.mat'),\n",
    "    os.path.join(project_folder, 'dataset3.mat')\n",
    "]\n",
    "\n",
    "# =========================================================\n",
    "#  Import datasets using helper function\n",
    "# =========================================================\n",
    "def import_data(mat_files):\n",
    "    datasets = {}\n",
    "    for f in mat_files:\n",
    "        name = os.path.splitext(os.path.basename(f))[0]\n",
    "        try:\n",
    "            data = sio.loadmat(f)\n",
    "            datasets[name] = data\n",
    "            print(f\"Loaded {f} using scipy.io\")\n",
    "        except NotImplementedError:\n",
    "            try:\n",
    "                with h5py.File(f, 'r') as hf:\n",
    "                    datasets[name] = {k: hf[k][:] for k in hf.keys()}\n",
    "                print(f\"Loaded {f} using h5py\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read {f}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {f}: {e}\")\n",
    "    return datasets\n",
    "\n",
    "datasets = import_data(mat_files)\n",
    "print(\"\\nAll loaded datasets:\", list(datasets.keys()))\n",
    "\n",
    "# =========================================================\n",
    "#  Combine data into X (sensors) and Y (positions)\n",
    "# =========================================================\n",
    "def combine_data(datasets):\n",
    "    X_list, y_list = [], []\n",
    "    for name, data in datasets.items():\n",
    "        sensor_key = [k for k in data.keys() if 'mag_sensors' in k][0]\n",
    "        position_key = [k for k in data.keys() if 'tip_position' in k][0]\n",
    "        X_list.append(np.array(data[sensor_key]))\n",
    "        y_list.append(np.array(data[position_key]))\n",
    "        print(f\" {name}: {sensor_key} {X_list[-1].shape}, {position_key} {y_list[-1].shape}\")\n",
    "    X = np.vstack(X_list)\n",
    "    y = np.vstack(y_list)\n",
    "    print(f\"\\n Combined shapes -> X: {X.shape}, y: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "X, Y = combine_data(datasets)\n",
    "\n",
    "# =========================================================\n",
    "#  Optional: create DataFrame for convenience\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "sensor_cols = [f'sensor_{i+1}' for i in range(X.shape[1])]\n",
    "coord_cols = ['x', 'y', 'z']\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    np.hstack([X, Y]),\n",
    "    columns=sensor_cols + coord_cols\n",
    ")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# =========================================================\n",
    "#  Generate feature combinations (optional for subset search)\n",
    "# =========================================================\n",
    "n_features = len(sensor_cols)\n",
    "all_combinations = []\n",
    "for k in range(n_features - 2, n_features + 1):\n",
    "    all_combinations.extend(combinations(sensor_cols, k))\n",
    "\n",
    "print(f\"Total subsets with almost all sensors: {len(all_combinations)}\")\n",
    "print(\"Example of first subsets:\", all_combinations[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4aeab118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sensor_1  sensor_2  sensor_3  sensor_4  sensor_5  sensor_6  sensor_7  \\\n",
      "0   15544.0   15471.0   13962.0   13618.0   13691.0   13941.0   15588.0   \n",
      "1   14100.0   14144.0   14144.0   13692.0   13338.0   13517.0   13987.0   \n",
      "2   14240.0   14266.0   14445.0   13859.0   13618.0   13756.0   14080.0   \n",
      "3   14956.0   14949.0   14123.0   13707.0   13765.0   13954.0   14554.0   \n",
      "4   14196.0   14246.0   16177.0   14830.0   13836.0   13911.0   13988.0   \n",
      "\n",
      "   sensor_8  sensor_9  sensor_10  sensor_11  sensor_12          x          y  \\\n",
      "0   15234.0   14156.0    13758.0    14098.0    14296.0  71.417546  -4.340464   \n",
      "1   13951.0   14162.0    13774.0    13784.0    13931.0  24.514493  47.778850   \n",
      "2   14036.0   14509.0    13958.0    13996.0    14122.0   2.951708  24.908397   \n",
      "3   14404.0   14208.0    13798.0    14127.0    14282.0  46.508669  -0.243722   \n",
      "4   13984.0   15788.0    14690.0    14185.0    14257.0 -41.687301  34.183930   \n",
      "\n",
      "            z  \n",
      "0   76.317760  \n",
      "1  110.173076  \n",
      "2  112.971805  \n",
      "3  100.567155  \n",
      "4   90.795163  \n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Create DataFrame with sensors and coordinates\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "# Sensor and coordinate column names\n",
    "sensor_cols = [f'sensor_{i+1}' for i in range(X.shape[1])]\n",
    "coord_cols = ['x', 'y', 'z']\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(\n",
    "    np.hstack([X, Y]),  # stack sensors and positions\n",
    "    columns=sensor_cols + coord_cols\n",
    ")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9515b6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de subsets com quase todos os sensores: 79\n",
      "Exemplo de primeiros subsets: [('sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10'), ('sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11'), ('sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_12'), ('sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_10', 'sensor_11'), ('sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_10', 'sensor_12')]\n"
     ]
    }
   ],
   "source": [
    "# Gerar combinações de quase todos os sensores \n",
    "n_features = len(sensor_cols)\n",
    "all_combinations = []\n",
    "for k in range(n_features - 2, n_features + 1):  # subsets of size 10, 11, 12\n",
    "    all_combinations.extend(combinations(sensor_cols, k))\n",
    "\n",
    "print(f\"Total de subsets com quase todos os sensores: {len(all_combinations)}\")\n",
    "print(\"Exemplo de primeiros subsets:\", all_combinations[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c0523cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sensor_1  sensor_2  sensor_3  sensor_4  sensor_5  sensor_6  sensor_7  \\\n",
      "0   15544.0   15471.0   13962.0   13618.0   13691.0   13941.0   15588.0   \n",
      "1   14100.0   14144.0   14144.0   13692.0   13338.0   13517.0   13987.0   \n",
      "2   14240.0   14266.0   14445.0   13859.0   13618.0   13756.0   14080.0   \n",
      "3   14956.0   14949.0   14123.0   13707.0   13765.0   13954.0   14554.0   \n",
      "4   14196.0   14246.0   16177.0   14830.0   13836.0   13911.0   13988.0   \n",
      "\n",
      "   sensor_8  sensor_9  sensor_10  sensor_11  sensor_12          x          y  \\\n",
      "0   15234.0   14156.0    13758.0    14098.0    14296.0  71.417546  -4.340464   \n",
      "1   13951.0   14162.0    13774.0    13784.0    13931.0  24.514493  47.778850   \n",
      "2   14036.0   14509.0    13958.0    13996.0    14122.0   2.951708  24.908397   \n",
      "3   14404.0   14208.0    13798.0    14127.0    14282.0  46.508669  -0.243722   \n",
      "4   13984.0   15788.0    14690.0    14185.0    14257.0 -41.687301  34.183930   \n",
      "\n",
      "            z  \n",
      "0   76.317760  \n",
      "1  110.173076  \n",
      "2  112.971805  \n",
      "3  100.567155  \n",
      "4   90.795163  \n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Create DataFrame with sensors and coordinates\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "# Sensor and coordinate column names\n",
    "sensor_cols = [f'sensor_{i+1}' for i in range(X.shape[1])]\n",
    "coord_cols = ['x', 'y', 'z']\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(\n",
    "    np.hstack([X, Y]),  # stack sensors and positions\n",
    "    columns=sensor_cols + coord_cols\n",
    ")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00be9c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset de features: ('sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10')\n",
      "   sensor_1  sensor_2  sensor_3  sensor_4  sensor_5  sensor_6  sensor_7  \\\n",
      "0   15544.0   15471.0   13962.0   13618.0   13691.0   13941.0   15588.0   \n",
      "1   14100.0   14144.0   14144.0   13692.0   13338.0   13517.0   13987.0   \n",
      "2   14240.0   14266.0   14445.0   13859.0   13618.0   13756.0   14080.0   \n",
      "3   14956.0   14949.0   14123.0   13707.0   13765.0   13954.0   14554.0   \n",
      "4   14196.0   14246.0   16177.0   14830.0   13836.0   13911.0   13988.0   \n",
      "\n",
      "   sensor_8  sensor_9  sensor_10  \n",
      "0   15234.0   14156.0    13758.0  \n",
      "1   13951.0   14162.0    13774.0  \n",
      "2   14036.0   14509.0    13958.0  \n",
      "3   14404.0   14208.0    13798.0  \n",
      "4   13984.0   15788.0    14690.0  \n"
     ]
    }
   ],
   "source": [
    "#preparação para treino de modelos\n",
    "y = df[coord_cols]\n",
    "\n",
    "# Exemplo: primeiro subset\n",
    "subset = all_combinations[0]\n",
    "X_subset = df[list(subset)]\n",
    "print(\"Subset de features:\", subset)\n",
    "print(X_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23dec210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Train Loss: 0.000056, Val Loss: 0.001469\n",
      "Epoch [20/50], Train Loss: 0.000035, Val Loss: 0.000919\n",
      "Epoch [30/50], Train Loss: 0.000026, Val Loss: 0.000798\n",
      "Epoch [40/50], Train Loss: 0.000019, Val Loss: 0.000498\n",
      "Epoch [50/50], Train Loss: 0.000016, Val Loss: 0.000420\n",
      " Latent features extracted. Original dim: 12 → Latent dim: 10\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "#  AUTOENCODER FEATURE EXTRACTION BEFORE anfis\n",
    "# =========================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data for AE training\n",
    "X_train_ae, X_val_ae = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale input data\n",
    "scaler_ae = StandardScaler()\n",
    "X_train_scaled = scaler_ae.fit_transform(X_train_ae)\n",
    "X_val_scaled = scaler_ae.transform(X_val_ae)\n",
    "\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_val_t = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Define Autoencoder\n",
    "# ---------------------------------------------------------\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=6):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Train Autoencoder\n",
    "# ---------------------------------------------------------\n",
    "input_dim = X.shape[1]\n",
    "latent_dim = 10  # you can tune this\n",
    "ae = Autoencoder(input_dim, latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ae.parameters(), lr=1e-3)\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ae.train()\n",
    "    permutation = torch.randperm(X_train_t.size(0))\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, X_train_t.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x = X_train_t[indices]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ae(batch_x)\n",
    "        loss = criterion(outputs, batch_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        ae.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = criterion(ae(X_val_t), X_val_t).item()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_loss/len(X_train_t):.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Extract latent features for entire dataset\n",
    "# ---------------------------------------------------------\n",
    "ae.eval()\n",
    "with torch.no_grad():\n",
    "    X_torch = torch.tensor(scaler_ae.transform(X), dtype=torch.float32)\n",
    "    X_latent_t = ae.encoder(X_torch)\n",
    "    X_latent = X_latent_t.numpy()\n",
    "\n",
    "print(f\" Latent features extracted. Original dim: {X.shape[1]} → Latent dim: {X_latent.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94df2a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating feature subsets for coordinate 1 (ANFIS)...\n",
      "Top 3 subsets for coordinate 1:\n",
      "Features: (0, 1, 2, 3, 5, 6, 7, 8, 9), MSE: 0.352413\n",
      "Features: (0, 2, 3, 4, 5, 6, 7, 8, 9), MSE: 0.432333\n",
      "Features: (0, 1, 3, 4, 5, 6, 7, 8, 9), MSE: 0.478384\n",
      "\n",
      " Evaluating feature subsets for coordinate 2 (ANFIS)...\n",
      "Top 3 subsets for coordinate 2:\n",
      "Features: (0, 1, 3, 4, 5, 6, 7, 8, 9), MSE: 0.412333\n",
      "Features: (0, 1, 2, 3, 4, 5, 6, 8, 9), MSE: 0.480278\n",
      "Features: (0, 1, 3, 5, 6, 7, 8, 9), MSE: 0.499492\n",
      "\n",
      " Evaluating feature subsets for coordinate 3 (ANFIS)...\n",
      "Top 3 subsets for coordinate 3:\n",
      "Features: (0, 1, 2, 3, 4, 5, 6, 7, 9), MSE: 0.431533\n",
      "Features: (0, 1, 2, 3, 5, 6, 7, 8, 9), MSE: 0.541953\n",
      "Features: (0, 1, 2, 3, 4, 7, 8, 9), MSE: 0.564060\n",
      "\n",
      " Parameter Grid Search for coordinate 1 (ANFIS)...\n",
      "\n",
      "Retraining best model on combined train+val...\n",
      "Epoch 1/80 Train MSE: 1.220117\n",
      "Epoch 10/80 Train MSE: 0.430053\n",
      "Epoch 20/80 Train MSE: 0.188494\n",
      "Epoch 30/80 Train MSE: 0.070825\n",
      "Epoch 40/80 Train MSE: 0.036609\n",
      "Epoch 50/80 Train MSE: 0.026629\n",
      "Epoch 60/80 Train MSE: 0.020430\n",
      "Epoch 70/80 Train MSE: 0.016940\n",
      "Epoch 80/80 Train MSE: 0.015116\n",
      "\n",
      " Best ANFIS model for coordinate 1:\n",
      "{'coord': 1, 'best_subset': (0, 2, 3, 4, 5, 6, 7, 8, 9), 'n_clusters': 4, 'lr': 0.01, 'epochs': 80, 'val_metrics': {'MSE': 0.01589626632630825, 'RMSE': 0.12608039628073925, 'MAE': 0.0987204909324646, 'R2': 0.984197199344635, 'NRMSE(%)': np.float32(2.987587)}, 'test_metrics': {'MSE': 0.014982849359512329, 'RMSE': 0.12240444991711832, 'MAE': 0.09365935623645782, 'R2': 0.9849966764450073, 'NRMSE(%)': np.float32(2.9693625)}}\n",
      "\n",
      " Parameter Grid Search for coordinate 2 (ANFIS)...\n",
      "\n",
      "Retraining best model on combined train+val...\n",
      "Epoch 1/80 Train MSE: 1.270842\n",
      "Epoch 10/80 Train MSE: 0.387107\n",
      "Epoch 20/80 Train MSE: 0.130073\n",
      "Epoch 30/80 Train MSE: 0.063168\n",
      "Epoch 40/80 Train MSE: 0.043919\n",
      "Epoch 50/80 Train MSE: 0.024825\n",
      "Epoch 60/80 Train MSE: 0.019972\n",
      "Epoch 70/80 Train MSE: 0.016307\n",
      "Epoch 80/80 Train MSE: 0.014184\n",
      "\n",
      " Best ANFIS model for coordinate 2:\n",
      "{'coord': 2, 'best_subset': (0, 1, 2, 3, 4, 5, 6, 8, 9), 'n_clusters': 5, 'lr': 0.01, 'epochs': 80, 'val_metrics': {'MSE': 0.016327515244483948, 'RMSE': 0.12777916592498148, 'MAE': 0.097173310816288, 'R2': 0.9830288887023926, 'NRMSE(%)': np.float32(2.9950762)}, 'test_metrics': {'MSE': 0.0142141692340374, 'RMSE': 0.11922319083985883, 'MAE': 0.09286332130432129, 'R2': 0.9865280985832214, 'NRMSE(%)': np.float32(2.760942)}}\n",
      "\n",
      " Parameter Grid Search for coordinate 3 (ANFIS)...\n",
      "\n",
      "Retraining best model on combined train+val...\n",
      "Epoch 1/80 Train MSE: 0.934249\n",
      "Epoch 10/80 Train MSE: 0.425082\n",
      "Epoch 20/80 Train MSE: 0.301489\n",
      "Epoch 30/80 Train MSE: 0.210192\n",
      "Epoch 40/80 Train MSE: 0.171266\n",
      "Epoch 50/80 Train MSE: 0.145813\n",
      "Epoch 60/80 Train MSE: 0.128052\n",
      "Epoch 70/80 Train MSE: 0.114310\n",
      "Epoch 80/80 Train MSE: 0.103108\n",
      "\n",
      " Best ANFIS model for coordinate 3:\n",
      "{'coord': 3, 'best_subset': (0, 1, 2, 3, 4, 5, 6, 7, 9), 'n_clusters': 5, 'lr': 0.01, 'epochs': 80, 'val_metrics': {'MSE': 0.11589685827493668, 'RMSE': 0.3404362763792024, 'MAE': 0.2603405714035034, 'R2': 0.8778623342514038, 'NRMSE(%)': np.float32(6.7751102)}, 'test_metrics': {'MSE': 0.10911398380994797, 'RMSE': 0.33032405878159704, 'MAE': 0.2600956857204437, 'R2': 0.8880140781402588, 'NRMSE(%)': np.float32(6.5831385)}}\n",
      "\n",
      "\n",
      "ANFIS training + evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "#  ANFIS MODEL USING AUTOENCODER LATENT FEATURES\n",
    "#  \n",
    "# =========================================================\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "import random, math, time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cpu\")  \n",
    "\n",
    "# =========================================================\n",
    "#  Normalize target coordinates \n",
    "# =========================================================\n",
    "y_scaler = StandardScaler()\n",
    "Y_scaled = y_scaler.fit_transform(Y)\n",
    "\n",
    "# =========================================================\n",
    "#  ANFIS architecture\n",
    "# =========================================================\n",
    "class ANFIS(nn.Module):\n",
    "    def __init__(self, n_inputs, n_rules, centers, sigmas):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_rules = n_rules\n",
    "        self.centers = nn.Parameter(centers.clone().float().to(device))\n",
    "        self.sigmas = nn.Parameter(sigmas.clone().float().to(device))\n",
    "        self.consequents = nn.Parameter(torch.randn(n_rules, n_inputs + 1) * 0.1)\n",
    "\n",
    "    def gaussian_mf(self, x):\n",
    "        diff = (x.unsqueeze(1) - self.centers.unsqueeze(0)) / (self.sigmas.unsqueeze(0) + 1e-9)\n",
    "        g = torch.exp(-0.5 * (diff ** 2).sum(dim=-1))\n",
    "        return g\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        firing = self.gaussian_mf(x)\n",
    "        norm = firing / (firing.sum(dim=1, keepdim=True) + 1e-9)\n",
    "        x_aug = torch.cat([x, torch.ones(batch, 1, device=x.device)], dim=1)\n",
    "        rule_out = torch.matmul(x_aug, self.consequents.t())\n",
    "        y = (norm * rule_out).sum(dim=1, keepdim=True)\n",
    "        return y, norm, rule_out\n",
    "\n",
    "# =========================================================\n",
    "#  Helper: initialize centers/sigmas using KMeans\n",
    "# =========================================================\n",
    "def compute_centers_sigmas(X_np, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_np)\n",
    "    centers = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "    sigmas_list = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = X_np[kmeans.labels_ == i]\n",
    "        if len(cluster_points) > 1:\n",
    "            sigma = np.std(cluster_points, axis=0)\n",
    "        else:\n",
    "            sigma = np.std(X_np, axis=0) * 0.5\n",
    "        sigma[sigma == 0] = 1e-2\n",
    "        sigmas_list.append(sigma)\n",
    "    sigmas = torch.tensor(np.stack(sigmas_list), dtype=torch.float32)\n",
    "    return centers, sigmas\n",
    "\n",
    "# =========================================================\n",
    "#  ANFIS training loop\n",
    "# =========================================================\n",
    "def train_anfis(model, X_train_t, y_train_t, epochs=50, lr=1e-3, verbose=False):\n",
    "    model.to(device)\n",
    "    X_train_t, y_train_t = X_train_t.to(device), y_train_t.to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        y_pred, _, _ = model(X_train_t)\n",
    "        loss = mse_loss(y_pred, y_train_t)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if verbose and ((epoch + 1) % 10 == 0 or epoch == 0):\n",
    "            print(f\"Epoch {epoch+1}/{epochs} Train MSE: {loss.item():.6f}\")\n",
    "    return model\n",
    "\n",
    "# =========================================================\n",
    "#  Evaluation metrics\n",
    "# =========================================================\n",
    "def evaluate_metrics_torch(y_true_t, y_pred_t):\n",
    "    y_true = y_true_t.detach().cpu().numpy().ravel()\n",
    "    y_pred = y_pred_t.detach().cpu().numpy().ravel()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    nrmse = rmse / (y_true.max() - y_true.min()) if (y_true.max() - y_true.min()) != 0 else float('nan')\n",
    "    return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2, \"NRMSE(%)\": nrmse * 100}\n",
    "\n",
    "# =========================================================\n",
    "#  Stage 1 – Feature subset evaluation\n",
    "# =========================================================\n",
    "def eval_anfis_subset(X_train_t, y_train_t, X_val_t, y_val_t,\n",
    "                      n_clusters=3, lr=1e-3, epochs=60):\n",
    "    centers, sigmas = compute_centers_sigmas(X_train_t.numpy(), n_clusters)\n",
    "    model = ANFIS(X_train_t.shape[1], n_clusters, centers, sigmas)\n",
    "    train_anfis(model, X_train_t, y_train_t, epochs=epochs, lr=lr)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_val, _, _ = model(X_val_t.to(device))\n",
    "    mse = float(((y_val_t.to(device) - y_pred_val)**2).mean().cpu().numpy())\n",
    "    return mse\n",
    "\n",
    "def evaluate_feature_subsets_anfis(X, Y, subset_sizes=None, max_subsets_per_size=10):\n",
    "    if subset_sizes is None:\n",
    "        subset_sizes = [X.shape[1]-2, X.shape[1]-1]\n",
    "    n_features = X.shape[1]\n",
    "    results = []\n",
    "    for size in subset_sizes:\n",
    "        feature_combinations = list(itertools.combinations(range(n_features), size))\n",
    "        if len(feature_combinations) > max_subsets_per_size:\n",
    "            feature_combinations = random.sample(feature_combinations, max_subsets_per_size)\n",
    "        for subset in feature_combinations:\n",
    "            X_sub = X[:, subset]\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_sub, Y, test_size=0.2, random_state=42)\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_val = scaler.transform(X_val)\n",
    "            X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "            y_train_t = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "            X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "            y_val_t = torch.tensor(y_val.reshape(-1, 1), dtype=torch.float32)\n",
    "            n_clusters = min(5, max(2, len(subset)))\n",
    "            mse = eval_anfis_subset(X_train_t, y_train_t, X_val_t, y_val_t, n_clusters=n_clusters)\n",
    "            results.append({\"features\": subset, \"metric\": mse})\n",
    "    results = sorted(results, key=lambda x: x[\"metric\"])\n",
    "    return results\n",
    "\n",
    "# =========================================================\n",
    "#  Stage 2 – Main loop (grid search + retrain)\n",
    "# =========================================================\n",
    "subset_results_all = []\n",
    "for coord in range(3):\n",
    "    print(f\"\\n Evaluating feature subsets for coordinate {coord+1} (ANFIS)...\")\n",
    "    res = evaluate_feature_subsets_anfis(X_latent, Y_scaled[:, coord], max_subsets_per_size=8)\n",
    "    subset_results_all.append(res)\n",
    "    print(f\"Top 3 subsets for coordinate {coord+1}:\")\n",
    "    for r in res[:3]:\n",
    "        print(f\"Features: {r['features']}, MSE: {r['metric']:.6f}\")\n",
    "\n",
    "param_grid = {\"n_clusters\": [3, 4, 5], \"lr\": [1e-2, 1e-3], \"epochs\": [40, 80]}\n",
    "final_results = []\n",
    "\n",
    "for coord in range(3):\n",
    "    print(f\"\\n Parameter Grid Search for coordinate {coord+1} (ANFIS)...\")\n",
    "    top_subsets = [r[\"features\"] for r in subset_results_all[coord][:3]]\n",
    "    best_val_mse = float(\"inf\")\n",
    "    best_model_info = None\n",
    "\n",
    "    for subset in top_subsets:\n",
    "        X_sub = X_latent[:, subset]\n",
    "        y_coord = Y_scaled[:, coord]\n",
    "        X_train_full, X_test, y_train_full, y_test = train_test_split(X_sub, y_coord, test_size=0.2, random_state=42)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_t = torch.tensor(y_train.reshape(-1,1), dtype=torch.float32)\n",
    "        X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_t = torch.tensor(y_val.reshape(-1,1), dtype=torch.float32)\n",
    "        X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_t = torch.tensor(y_test.reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "        for ncl, lr, epochs in itertools.product(param_grid[\"n_clusters\"], param_grid[\"lr\"], param_grid[\"epochs\"]):\n",
    "            centers, sigmas = compute_centers_sigmas(X_train, ncl)\n",
    "            model = ANFIS(X_train.shape[1], ncl, centers, sigmas)\n",
    "            train_anfis(model, X_train_t, y_train_t, epochs=epochs, lr=lr)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_val, _, _ = model(X_val_t)\n",
    "            val_metrics = evaluate_metrics_torch(y_val_t, y_pred_val)\n",
    "            if val_metrics[\"MSE\"] < best_val_mse:\n",
    "                best_val_mse = val_metrics[\"MSE\"]\n",
    "                best_model_info = {\n",
    "                    \"coord\": coord+1,\n",
    "                    \"best_subset\": subset,\n",
    "                    \"n_clusters\": ncl,\n",
    "                    \"lr\": lr,\n",
    "                    \"epochs\": epochs,\n",
    "                    \"val_metrics\": val_metrics,\n",
    "                    \"scaler\": scaler,\n",
    "                }\n",
    "\n",
    "    # Retrain best model on train+val\n",
    "    print(\"\\nRetraining best model on combined train+val...\")\n",
    "    subset = best_model_info[\"best_subset\"]\n",
    "    X_sub_all = X_latent[:, subset]\n",
    "    y_coord_all = Y_scaled[:, coord]\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(X_sub_all, y_coord_all, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_trainval_s = scaler.fit_transform(X_trainval)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    X_trainval_t = torch.tensor(X_trainval_s, dtype=torch.float32)\n",
    "    y_trainval_t = torch.tensor(y_trainval.reshape(-1,1), dtype=torch.float32)\n",
    "    X_test_t = torch.tensor(X_test_s, dtype=torch.float32)\n",
    "    y_test_t = torch.tensor(y_test.reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "    centers, sigmas = compute_centers_sigmas(X_trainval_s, best_model_info[\"n_clusters\"])\n",
    "    best_model = ANFIS(X_trainval_t.shape[1], best_model_info[\"n_clusters\"], centers, sigmas)\n",
    "    train_anfis(best_model, X_trainval_t, y_trainval_t, epochs=best_model_info[\"epochs\"], lr=best_model_info[\"lr\"], verbose=True)\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_test, _, _ = best_model(X_test_t)\n",
    "    test_metrics = evaluate_metrics_torch(y_test_t, y_pred_test)\n",
    "\n",
    "    best_model_info[\"test_metrics\"] = test_metrics\n",
    "    best_model_info[\"model\"] = best_model\n",
    "    final_results.append(best_model_info)\n",
    "\n",
    "    print(f\"\\n Best ANFIS model for coordinate {coord+1}:\")\n",
    "    print({\n",
    "        \"coord\": best_model_info[\"coord\"],\n",
    "        \"best_subset\": best_model_info[\"best_subset\"],\n",
    "        \"n_clusters\": best_model_info[\"n_clusters\"],\n",
    "        \"lr\": best_model_info[\"lr\"],\n",
    "        \"epochs\": best_model_info[\"epochs\"],\n",
    "        \"val_metrics\": best_model_info[\"val_metrics\"],\n",
    "        \"test_metrics\": best_model_info[\"test_metrics\"]\n",
    "    })\n",
    "\n",
    "print(\"\\n\\nANFIS training + evaluation complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-automation2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d70495b7-a41b-4def-aefa-407cc9978df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bfa2ce8-35d6-4fe4-948c-bf13158e312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(mat_files):\n",
    "    # Dictionary to store the loaded data\n",
    "    datasets = {}\n",
    "\n",
    "    for f in mat_files:\n",
    "        name = os.path.splitext(os.path.basename(f))[0]  # filename without extension\n",
    "        try:\n",
    "            data = sio.loadmat(f)\n",
    "            datasets[name] = data\n",
    "            print(f\"Loaded {f} using scipy.io\")\n",
    "        except NotImplementedError:\n",
    "            try:\n",
    "                with h5py.File(f, 'r') as hf:\n",
    "                    datasets[name] = {k: hf[k][:] for k in hf.keys()}\n",
    "                print(f\"Loaded {f} using h5py\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read {f}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {f}: {e}\")\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ff0e5e-c09f-4aa7-b789-27832f6d5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(datasets):\n",
    "    \"\"\"\n",
    "    Combine all datasets into single X (sensors) and y (positions) arrays.\n",
    "    Expects each dataset to have keys like:\n",
    "      'mag_sensors', 'tip_position' (or mag_sensors2, tip_position2, etc.)\n",
    "    \"\"\"\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for name, data in datasets.items():\n",
    "        # find the sensor and position keys automatically\n",
    "        sensor_key = [k for k in data.keys() if 'mag_sensors' in k][0]\n",
    "        position_key = [k for k in data.keys() if 'tip_position' in k][0]\n",
    "\n",
    "        X_list.append(numpy.array(data[sensor_key]))\n",
    "        y_list.append(numpy.array(data[position_key]))\n",
    "\n",
    "        print(f\"âœ… {name}: {sensor_key} {X_list[-1].shape}, {position_key} {y_list[-1].shape}\")\n",
    "\n",
    "    # stack all vertically\n",
    "    X = numpy.vstack(X_list)\n",
    "    y = numpy.vstack(y_list)\n",
    "\n",
    "    print(f\"\\nðŸ“¦ Combined shapes -> X: {X.shape}, y: {y.shape}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d3d0de1-c676-4d85-ab32-5580c6827ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, train_pct=0.7, val_pct=0.15, test_pct=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataset (X, y) into train, validation, and test sets.\n",
    "    Works for multi-output regression (e.g., 3D positions).\n",
    "    \"\"\"\n",
    "    assert abs(train_pct + val_pct + test_pct - 1.0) < 1e-6, \"Percents must sum to 1\"\n",
    "\n",
    "    numpy.random.seed(random_state)\n",
    "    N = X.shape[0]\n",
    "    indices = numpy.random.permutation(N)\n",
    "\n",
    "    train_end = int(train_pct * N)\n",
    "    val_end = train_end + int(val_pct * N)\n",
    "\n",
    "    train_idx = indices[:train_end]\n",
    "    val_idx = indices[train_end:val_end]\n",
    "    test_idx = indices[val_end:]\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_val, y_val = X[val_idx], y[val_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    print(f\"ðŸ“Š Split -> Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf67046-fb37-44ba-8972-200028d4e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Standardize inputs (X)\n",
    "    x_scaler = StandardScaler()\n",
    "    X_train = x_scaler.fit_transform(X_train)\n",
    "    X_val = x_scaler.transform(X_val)\n",
    "    X_test = x_scaler.transform(X_test)\n",
    "\n",
    "    # Standardize outputs (y)\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train = y_scaler.fit_transform(y_train)\n",
    "    y_val = y_scaler.transform(y_val)\n",
    "    y_test = y_scaler.transform(y_test)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948567d0-2764-4efa-b94a-2da9a231be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_delta_dataset(mag_sensors):\n",
    "    \"\"\"\n",
    "    Replace each pair of magnetic sensors with their average and delta,\n",
    "    keeping the same overall dataset shape and order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mag_sensors : numpy.ndarray\n",
    "        Shape (N, 12), dtype numeric. \n",
    "        Sensors are assumed to be paired as (1,2), (3,4), ..., (11,12).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_dataset : numpy.ndarray\n",
    "        Shape (N, 12), where columns are ordered as:\n",
    "        [avg_1, delta_1, avg_2, delta_2, ..., avg_6, delta_6]\n",
    "    \"\"\"\n",
    "    if mag_sensors.shape[1] != 12:\n",
    "        raise ValueError(\"Expected 12 sensor columns (got {})\".format(mag_sensors.shape[1]))\n",
    "\n",
    "    N = mag_sensors.shape[0]\n",
    "    new_dataset = numpy.zeros_like(mag_sensors, dtype=float)\n",
    "\n",
    "    for i in range(0, 12, 2):\n",
    "        avg = 0.5 * (mag_sensors[:, i] + mag_sensors[:, i + 1])\n",
    "        delta = mag_sensors[:, i] - mag_sensors[:, i + 1]\n",
    "        pair_idx = i // 2\n",
    "        new_dataset[:, 2 * pair_idx] = avg\n",
    "        new_dataset[:, 2 * pair_idx + 1] = delta\n",
    "\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d50b1e-17a0-429e-a5c2-867355b0a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def select_features(X, y, top_k=None, threshold=None, scale=True):\n",
    "    \"\"\"\n",
    "    Select the most relevant input features for regression tasks using mutual information.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Input features, shape (n_samples, n_features)\n",
    "        y (ndarray): Targets, shape (n_samples,) or (n_samples, n_outputs)\n",
    "        top_k (int): Keep the top_k highest-score features (optional)\n",
    "        threshold (float): Keep features with score >= threshold (optional)\n",
    "        scale (bool): Whether to standardize inputs before scoring\n",
    "\n",
    "    Returns:\n",
    "        X_selected (ndarray): Reduced feature matrix\n",
    "        selected_indices (list[int]): Indices of selected features\n",
    "        scores (ndarray): Importance scores for all features\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle multi-output target by averaging scores\n",
    "    y_ = y if y.ndim == 1 else np.mean(y, axis=1)\n",
    "\n",
    "    # Optional standardization (important for continuous variables)\n",
    "    if scale:\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Mutual information measures nonlinear dependency\n",
    "    scores = mutual_info_regression(X, y_)\n",
    "    scores = np.nan_to_num(scores)\n",
    "\n",
    "    # Rank features\n",
    "    ranked_indices = np.argsort(scores)[::-1]\n",
    "\n",
    "    # Select top_k or threshold\n",
    "    if top_k is not None:\n",
    "        selected_indices = ranked_indices[:top_k]\n",
    "    elif threshold is not None:\n",
    "        selected_indices = np.where(scores >= threshold)[0]\n",
    "    else:\n",
    "        # Default: keep all features sorted by importance\n",
    "        selected_indices = ranked_indices\n",
    "\n",
    "    X_selected = X[:, selected_indices]\n",
    "    return X_selected, selected_indices, scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
